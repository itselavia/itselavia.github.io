<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://itselavia.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://itselavia.github.io/" rel="alternate" type="text/html" /><updated>2021-02-16T08:19:23-08:00</updated><id>https://itselavia.github.io/feed.xml</id><title type="html">Learning Orbit</title><subtitle>A technical blog documenting topics about cloud, security and DevOps</subtitle><entry><title type="html">Dynamically Update Pod’s Environment Variables</title><link href="https://itselavia.github.io/2021/02/09/dynamic-update-env-vars/" rel="alternate" type="text/html" title="Dynamically Update Pod’s Environment Variables" /><published>2021-02-09T04:00:00-08:00</published><updated>2021-02-09T04:00:00-08:00</updated><id>https://itselavia.github.io/2021/02/09/dynamic-update-env-vars</id><content type="html" xml:base="https://itselavia.github.io/2021/02/09/dynamic-update-env-vars/">&lt;p&gt;
It's a widely accepted practice to inject the environment variables into pods from ConfigMaps. This allows for more dynamic deployment of applications where the runtime properties need not be hardcoded into the application. Once the ConfigMap with the required key value pairs is created, all its entries can be used as environment variables for a pod. This can be configured as follows: 
&lt;?prettify?&gt; &lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;
apiVersion: v1
kind: Pod
metadata:
  name: env-test-pod
  labels:
    app: env-test
spec:
  containers:
    - name: env-test-container
      image: itselavia/dynamic-update-env
      envFrom:
        - configMapRef:
            name: env-vars
&lt;/pre&gt;
&lt;p&gt;
But, there are some use cases where the the environment variables needs to be modified and the application needs to react to those changes. This creates a bit of an operational issue because Kubernetes does not automatically restart the pod or updates its environment variables if a referenced ConfigMap is updated.&lt;/p&gt;
&lt;p&gt;
However, Kubernetes does update the volume mounts of the pod if the ConfigMap is mounted to the pod. So the basic idea is alongwith setting the environment variables using envFrom and configMapRef, we also mount the ConfigMap to the pod at a specific directory. This is configured as follows: 
&lt;?prettify?&gt; &lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;
apiVersion: v1
kind: Pod
metadata:
  name: env-test-pod
  labels:
    app: env-test
spec:
  containers:
    - name: env-test-container
      image: itselavia/dynamic-update-env
      envFrom:
        - configMapRef:
            name: env-vars
      volumeMounts:
        - mountPath: /config
          name: env-volume
  volumes:
    - name: env-volume
      configMap:
        name: env-vars
&lt;/pre&gt;

&lt;p&gt;
We monitor the /config configuration directory for any changes. This example application uses &lt;a href=&quot;https://github.com/fsnotify/fsnotify&quot; target=&quot;_blank&quot;&gt;fsnotify&lt;/a&gt; which is a golang library which implements filesystem monitoring for many platforms. We also need to reprogram the application to monitor the configuration directory in the background and update the environment variables in the pod when any change event is received. The Go code snippet is shared below:
&lt;?prettify?&gt; &lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;
func main() {

  watcher, err := fsnotify.NewWatcher()
  if err != nil {
    fmt.Println(&quot;cannot initialize Watcher &quot;, err)
  }
  defer watcher.Close()

  // watcher will monitor the files in a background goroutine
  go func() {
    for {
      select {
      // reload the environment variables whenever changes are made in the /config directory
      case _, ok := &amp;lt;-watcher.Events:
        if !ok {
          return
        }
        reloadEnvVars()
      case err, ok := &amp;lt;-watcher.Errors:
        if !ok {
          fmt.Println(&quot;error from Watcher: &quot;, err)
          return
        }
      }
    }
  }()

  // monitor the /config directory
  err = watcher.Add(&quot;/config/&quot;)
  if err != nil {
    fmt.Println(&quot;error adding directory to Watcher&quot;, err)
  }
}
&lt;/pre&gt;
&lt;p&gt;
The overall steps are depicted in the below diagram. The source code is available at &lt;a href=&quot;https://github.com/itselavia/dynamic-update-configmap-env-vars&quot; target=&quot;_blank&quot;&gt;this GitHub repository&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;#&quot;&gt;
    &lt;img src=&quot;/img/flow-diagram.png&quot; alt=&quot;Flow Diagram for Dynamically Updating Environment Variables&quot; style=&quot;display: block;margin-left: auto;margin-right: auto;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption text-muted&quot;&gt;Flow Diagram for Dynamically Updating Environment Variables&lt;/span&gt;&lt;/p&gt;
&lt;h5 class=&quot;section-heading&quot;&gt;Demo&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Create a ConfigMap with environment variables KEY1=VAL1 and KEY2=VAL2&lt;/li&gt; 
&lt;pre class=&quot;prettyprint&quot;&gt;
kubectl apply -f https://raw.githubusercontent.com/itselavia/dynamic-update-configmap-env-vars/main/configmap.yaml
&lt;/pre&gt;

&lt;li&gt;Create a pod which refers the ConfigMap and mounts its contents to the /config directory inside the pod.&lt;/li&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;
kubectl apply -f https://raw.githubusercontent.com/itselavia/dynamic-update-configmap-env-vars/main/pod.yaml
&lt;/pre&gt;

&lt;li&gt;Create a Service to expose the pod&lt;/li&gt; 
&lt;pre class=&quot;prettyprint&quot;&gt;
kubectl apply -f https://raw.githubusercontent.com/itselavia/dynamic-update-configmap-env-vars/main/service.yaml
&lt;/pre&gt;

&lt;li&gt;Create a test pod to access the application&lt;/li&gt; 
&lt;pre class=&quot;prettyprint&quot;&gt;
kubectl run curl-test --image=radial/busyboxplus:curl -i --tty --rm
&lt;/pre&gt;

&lt;li&gt;Access the application by querying for the value of KEY1&lt;/li&gt; 
&lt;pre class=&quot;prettyprint&quot;&gt;
curl http://env-svc:8080/getEnvValue?var=KEY1
VAL1
&lt;/pre&gt;

&lt;li&gt;Edit the ConfigMap and change the value of KEY1&lt;/li&gt; 
&lt;pre class=&quot;prettyprint&quot;&gt;
kubectl edit cm env-vars
&lt;/pre&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;
apiVersion: v1
kind: ConfigMap
metadata:
    name: env-vars
data:
    KEY1: NEW_VAL1
    KEY2: VAL2
&lt;/pre&gt;

&lt;li&gt;Wait for 30-60 seconds and query for the value of KEY1&lt;/li&gt; 
&lt;pre class=&quot;prettyprint&quot;&gt;
curl http://env-svc:8080/getEnvValue?var=KEY1
NEW_VAL1
&lt;/pre&gt;
&lt;/ul&gt;
&lt;p&gt;
Note: The time period for synchronization between API server and Kubelet is set using the &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/b2b8c1f18d1056db23c1cd377d6dc3dd4fb4dcc1/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L105&quot; target=&quot;_blank&quot;&gt;--sync-frequency&lt;/a&gt; parameter to kubelet config 
&lt;/p&gt;</content><author><name>Akshay Elavia</name></author><summary type="html">It's a widely accepted practice to inject the environment variables into pods from ConfigMaps. This allows for more dynamic deployment of applications where the runtime properties need not be hardcoded into the application. Once the ConfigMap with the required key value pairs is created, all its entries can be used as environment variables for a pod. This can be configured as follows: apiVersion: v1 kind: Pod metadata: name: env-test-pod labels: app: env-test spec: containers: - name: env-test-container image: itselavia/dynamic-update-env envFrom: - configMapRef: name: env-vars But, there are some use cases where the the environment variables needs to be modified and the application needs to react to those changes. This creates a bit of an operational issue because Kubernetes does not automatically restart the pod or updates its environment variables if a referenced ConfigMap is updated. However, Kubernetes does update the volume mounts of the pod if the ConfigMap is mounted to the pod. So the basic idea is alongwith setting the environment variables using envFrom and configMapRef, we also mount the ConfigMap to the pod at a specific directory. This is configured as follows: apiVersion: v1 kind: Pod metadata: name: env-test-pod labels: app: env-test spec: containers: - name: env-test-container image: itselavia/dynamic-update-env envFrom: - configMapRef: name: env-vars volumeMounts: - mountPath: /config name: env-volume volumes: - name: env-volume configMap: name: env-vars</summary></entry><entry><title type="html">Automatically Deploy KubeEdge cluster on Google Cloud Platform</title><link href="https://itselavia.github.io/2021/01/02/deploy-kubeedge-gcp/" rel="alternate" type="text/html" title="Automatically Deploy KubeEdge cluster on Google Cloud Platform" /><published>2021-01-02T04:00:00-08:00</published><updated>2021-01-02T04:00:00-08:00</updated><id>https://itselavia.github.io/2021/01/02/deploy-kubeedge-gcp</id><content type="html" xml:base="https://itselavia.github.io/2021/01/02/deploy-kubeedge-gcp/">&lt;p&gt;
Kubernetes is slowly becoming the de-facto standard for managing workloads on large cluster of machines, especially in the cloud environment. Architectural principles of Kubernetes (like containerized workloads, efficient resource allocation and utilization, extensibility, remote deployment and management of applications) can be very well applied to Edge Computing. This is exactly what the &lt;a href=&quot;https://kubeedge.io/en/&quot; target=&quot;_blank&quot;&gt;KubeEdge&lt;/a&gt; project has implemented. According to the documentation, &quot;KubeEdge is an open source system for extending native containerized application orchestration capabilities to hosts at Edge.It is built upon kubernetes and provides fundamental infrastructure support for network, app. deployment and metadata synchronization between cloud and edge&quot;.
&lt;/p&gt;
&lt;p&gt;
KubeEdge works in tandem with the Kubernetes control plane to manage the edge nodes. The edge nodes are devices like Raspberry Pi (or any Linux based SBC) which run the containerized applications. KubeEdge has two main components: Cloudcore and Edgecore. Cloudcore is the collection of 3 components (CloudHub, EdgeController and DeviceController) which interact with the Kubernetes API server and manages the communication with edge nodes. Edgecore runs on the edge devices which syncs the device changes with cloud and manages the containerized applications.  
&lt;/p&gt;
&lt;p&gt;
To quicly start experimenting with KubeEdge, we need not actually procure hardware like Raspberry Pi. The Edgecore can be executed on modest virtual machines so that we can verify our applications/architecture/workflow before deploying them to physical devices. The following diagram shows the resources we are going to create for our test KubeEdge cluster:
&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;#&quot;&gt;
    &lt;img src=&quot;/img/deploy-kubeedge-gcp.png&quot; alt=&quot;Architecture Diagram of KubeEdge Cluster Deployment&quot; style=&quot;display: block;margin-left: auto;margin-right: auto;&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption text-muted&quot;&gt;Architecture Diagram of KubeEdge Cluster Deployment&lt;/span&gt;&lt;/p&gt;
&lt;h5 class=&quot;section-heading&quot;&gt;Infrastructure Components&lt;/h5&gt;
&lt;p&gt;
&lt;a href=&quot;https://github.com/itselavia/kubeedge-cluster-gcp&quot; target=&quot;_blank&quot;&gt;This GitHub repository&lt;/a&gt; contains the Terraform code to create this architecture. There are Terraform modules are written:
&lt;ul&gt;
&lt;li&gt;VPC&lt;/li&gt;
The module creates a VPC with 2 subnetworks, one for the cloud components and the other for the edge components
&lt;li&gt;Config Bucket&lt;/li&gt;
This is the bucket stores the configuration files and tokens which the Kubernetes and KubeEdge clusters need to share among each other. When the Kubernetes control plane is initialzed, the kubeconfig file is uploaded to the config bucket. The kubeconfig file is later downloaded by Cloudcore during KubeEdge cluster deployment, and also to the end user's machine from where Terraform is applied. Cloudcore also uploads the keadm token to the config bucket, which is later downloaded by the Edgecore during installation.
&lt;li&gt;Kubernetes Cluster&lt;/li&gt;
The Kubernetes cluster consists of 1 control plane node and n worker nodes. The cluster is created using kubeadm. These shell scripts install the cluster:
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/itselavia/kubeedge-cluster-gcp/blob/master/modules/kubernetes/scripts/controlplane_init.sh&quot;&gt;controlplane_init.sh: &lt;/a&gt; This will install Docker, kubectl, and kubeadm on the Kubernetes control plane node. It also initializes the control plane, install the Flannel pod network add on, and copies the generated kube config file to the config bucket&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/itselavia/kubeedge-cluster-gcp/blob/master/modules/kubernetes/scripts/worker_init.sh&quot;&gt;worker_init.sh: &lt;/a&gt; This will install Docker, kubectl and kubeadm on all the Kubernetes worker nodes. It also waits for the control plane to be available and joins the worker nodes to the cluster&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;KubeEdge Cluster&lt;/li&gt;
The KubeEdge cluster consists of 1 Cloudcore node which connects with the Kubernetes control plane. And n Edgecore nodes which represent edge devices. These shell scripts install the KubeEdge cluster:
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/itselavia/kubeedge-cluster-gcp/blob/master/modules/kubeedge/scripts/cloudcore_init.sh&quot;&gt;cloudcore_init.sh: &lt;/a&gt; This will download the kube config file from the config bucket, install keadm utility and initialize the KubeEdge cloudcore. It also uploads the keadm join token to the config bucket&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/itselavia/kubeedge-cluster-gcp/blob/master/modules/kubeedge/scripts/edgecore_init.sh&quot;&gt;edgecore_init.sh: &lt;/a&gt; This will install Docker and keadm on all the edge nodes. It waits for the KubeEdge cloudcore to be available and joins the edge nodes to the cluster&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;pre class=&quot;prettyprint&quot;&gt;
provider &quot;google&quot; {
  credentials = file(var.credentials_file_location)
  project     = var.project_name
  region      = var.region
  zone        = var.zone
}

module &quot;vpc&quot; {
  source   = &quot;./modules/vpc&quot;
  vpc_name = var.vpc_name
  region   = var.region
}

module &quot;config_bucket&quot; {
  source       = &quot;./modules/config_bucket&quot;
  project_name = var.project_name
}

module &quot;kubernetes&quot; {
  source                = &quot;./modules/kubernetes&quot;
  vpc_name              = var.vpc_name
  subnetwork_name       = module.vpc.subnetwork_1_name
  zone                  = var.zone
  k8s_worker_node_count = var.k8s_worker_node_count
  config_bucket         = module.config_bucket.config_bucket_url
}

module &quot;kubeedge&quot; {
  source                = &quot;./modules/kubeedge&quot;
  config_bucket         = module.config_bucket.config_bucket_url
  vpc_name              = var.vpc_name
  zone                  = var.zone
  subnetwork_name       = module.vpc.subnetwork_2_name
  edge_node_count       = var.edge_node_count
}
&lt;/pre&gt;
&lt;h5 class=&quot;section-heading&quot;&gt;Steps to Deploy the Cluster&lt;/h5&gt;
&lt;p&gt;
&lt;ol&gt;
&lt;li&gt;Create an account on Google Cloud Platform and create a project. This project is where all the infrastructure resources will be created.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to the &lt;a href=&quot;https://console.cloud.google.com/apis/credentials/serviceaccountkey/&quot; target=&quot;_blank&quot;&gt;service account key page in the Cloud Console&lt;/a&gt; and create a new service account with Project -&amp;gt; Owner role. Download the credentials file in JSON format.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enable the GCP APIs: Storage, Compute Engine, VPC, IAM. If you're enabling the APIs for the first time, wait ~20-30 minutes before applying Terraform. The GCP API activation does not take immediate effect.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Clone &lt;a href=&quot;https://github.com/itselavia/kubeedge-cluster-gcp/&quot; target=&quot;_blank&quot;&gt;this GitHub repository&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;cd in to the repository&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the file named “terraform.tfvars” with the following required parameters:
&lt;ol&gt;
&lt;li&gt;region: the GCP region where all the resources will be created&lt;/li&gt;
&lt;li&gt;vpc_name: the name for the virtual private cloud network&lt;/li&gt;
&lt;li&gt;zone: the GCP zone where all the resources will be created. The zone must be in the same region as declared in step 6.1&lt;/li&gt;
&lt;li&gt;project_name: name of the GCP project which was created in step 1&lt;/li&gt;
&lt;li&gt;k8s_worker_node_count: number of worker nodes to be created for the Kubernetes cluster&lt;/li&gt;
&lt;li&gt;edge_node_count: number of edge nodes to be created for the KubeEdge cluster&lt;/li&gt;
&lt;li&gt;credentials_file_location: path to the JSON credentials file downloaded in Step 2&lt;/li&gt;
&lt;/ol&gt;
Example terraform.tfvars file:
&lt;pre class=&quot;prettyprint&quot;&gt;
region                    = &quot;us-west2&quot;
vpc_name                  = &quot;kubeedge-infra&quot;
zone                      = &quot;us-west2-a&quot;
project_name              = &quot;deploy-kubeedge-gcp&quot;
k8s_worker_node_count     = 2
edge_node_count           = 4
credentials_file_location = &quot;~/Downloads/deploy-kubeedge-gcp-19242787b5a4.json&quot;
&lt;/pre&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Install Terraform by following the steps mentioned in the &lt;a href=&quot;https://learn.hashicorp.com/tutorials/terraform/install-cli/&quot; target=&quot;_blank&quot;&gt;official documentation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;mark&gt;terraform init&lt;/mark&gt; to initialize the and import the Terraform modules into the working directory&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;mark&gt;terraform plan&lt;/mark&gt; to view all the infrastructure components Terraform will create in Google Cloud&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;mark&gt;terraform apply --auto-approve&lt;/mark&gt; which will start creating the cluster infrastructure on GCP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The kubeconfig file for the Kubernetes cluster is downloaded to modules/kubernetes/config. Run the following command to setup KUBECONFIG
&lt;pre class=&quot;prettyprint&quot;&gt;
export KUBECONFIG=`pwd`/modules/kubernetes/config
&lt;/pre&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;mark&gt;kubectl get nodes&lt;/mark&gt; to see the worker nodes and edge nodes of the cluster&lt;/p&gt;&lt;/li&gt;
&lt;a href=&quot;#&quot;&gt;
    &lt;img src=&quot;/img/kubeedge-get-nodes.png&quot; alt=&quot;kubectl get nodes&quot; style=&quot;display: block;margin-left: auto;margin-right: auto;&quot; /&gt;
&lt;/a&gt;
&lt;li&gt;&lt;p&gt;If the nodes are not visible, it’s possible that the cluster setup is still in progress. To view the logs for cluster setup, ssh into the node via the GCP Virtual Machines UI, and tail the /var/log/user-data.log file on any node.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you want to add or remove any nodes from the existing cluster, simply edit the “terraform.tfvars” file with desired values for variables &lt;mark&gt;k8s_worker_node_count&lt;/mark&gt; or/and &lt;mark&gt;edge_node_count&lt;/mark&gt; and run &lt;mark&gt;terraform apply --auto-approve&lt;/mark&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Login to the GCP Console to view the created resources
&lt;a href=&quot;#&quot;&gt;
    &lt;img src=&quot;/img/kubeedge-gcp-console.png&quot; alt=&quot;Resources created in GCP&quot; style=&quot;display: block;margin-left: auto;margin-right: auto;&quot; /&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;</content><author><name>Akshay Elavia</name></author><summary type="html">Kubernetes is slowly becoming the de-facto standard for managing workloads on large cluster of machines, especially in the cloud environment. Architectural principles of Kubernetes (like containerized workloads, efficient resource allocation and utilization, extensibility, remote deployment and management of applications) can be very well applied to Edge Computing. This is exactly what the KubeEdge project has implemented. According to the documentation, &quot;KubeEdge is an open source system for extending native containerized application orchestration capabilities to hosts at Edge.It is built upon kubernetes and provides fundamental infrastructure support for network, app. deployment and metadata synchronization between cloud and edge&quot;. KubeEdge works in tandem with the Kubernetes control plane to manage the edge nodes. The edge nodes are devices like Raspberry Pi (or any Linux based SBC) which run the containerized applications. KubeEdge has two main components: Cloudcore and Edgecore. Cloudcore is the collection of 3 components (CloudHub, EdgeController and DeviceController) which interact with the Kubernetes API server and manages the communication with edge nodes. Edgecore runs on the edge devices which syncs the device changes with cloud and manages the containerized applications. To quicly start experimenting with KubeEdge, we need not actually procure hardware like Raspberry Pi. The Edgecore can be executed on modest virtual machines so that we can verify our applications/architecture/workflow before deploying them to physical devices. The following diagram shows the resources we are going to create for our test KubeEdge cluster: Architecture Diagram of KubeEdge Cluster Deployment Infrastructure Components This GitHub repository contains the Terraform code to create this architecture. There are Terraform modules are written: VPC The module creates a VPC with 2 subnetworks, one for the cloud components and the other for the edge components Config Bucket This is the bucket stores the configuration files and tokens which the Kubernetes and KubeEdge clusters need to share among each other. When the Kubernetes control plane is initialzed, the kubeconfig file is uploaded to the config bucket. The kubeconfig file is later downloaded by Cloudcore during KubeEdge cluster deployment, and also to the end user's machine from where Terraform is applied. Cloudcore also uploads the keadm token to the config bucket, which is later downloaded by the Edgecore during installation. Kubernetes Cluster The Kubernetes cluster consists of 1 control plane node and n worker nodes. The cluster is created using kubeadm. These shell scripts install the cluster: controlplane_init.sh: This will install Docker, kubectl, and kubeadm on the Kubernetes control plane node. It also initializes the control plane, install the Flannel pod network add on, and copies the generated kube config file to the config bucket worker_init.sh: This will install Docker, kubectl and kubeadm on all the Kubernetes worker nodes. It also waits for the control plane to be available and joins the worker nodes to the cluster KubeEdge Cluster The KubeEdge cluster consists of 1 Cloudcore node which connects with the Kubernetes control plane. And n Edgecore nodes which represent edge devices. These shell scripts install the KubeEdge cluster: cloudcore_init.sh: This will download the kube config file from the config bucket, install keadm utility and initialize the KubeEdge cloudcore. It also uploads the keadm join token to the config bucket edgecore_init.sh: This will install Docker and keadm on all the edge nodes. It waits for the KubeEdge cloudcore to be available and joins the edge nodes to the cluster provider &quot;google&quot; { credentials = file(var.credentials_file_location) project = var.project_name region = var.region zone = var.zone }</summary></entry><entry><title type="html">Automated deployment of MongoDB cluster to AWS using Terraform</title><link href="https://itselavia.github.io/2019/12/01/automated-mongodb-deployment/" rel="alternate" type="text/html" title="Automated deployment of MongoDB cluster to AWS using Terraform" /><published>2019-12-01T04:00:00-08:00</published><updated>2019-12-01T04:00:00-08:00</updated><id>https://itselavia.github.io/2019/12/01/automated-mongodb-deployment</id><content type="html" xml:base="https://itselavia.github.io/2019/12/01/automated-mongodb-deployment/">&lt;p&gt;Deploying a cluster is hard. Deploying a database cluster is harder. Deploying a database cluster repeatedly to the cloud is an unimaginable headache. Nowadays, IaC tools allows teams to automatically provision the complex intertwined cloud infrastructure in a deterministic manner. There can be multiple reasons why you would want to invest efforts into writing code for automating the deployment of a database cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performance testing your applications' access patterns by pointing them to your database&lt;/li&gt; 
&lt;li&gt;Documenting the custom configurations made while setting up the database&lt;/li&gt;
&lt;li&gt;Safely and quickly replicating environments for staging, testing, DR, etc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we'll look at deploying a MongoDB cluster to AWS with 1 primary (master) node and n secondary (data) nodes. We'll codify all our configurations with Terraform, shell scripts and Python scripts, to launch and bootstrap our cluster. This is the architecture diagram of the deployment:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#&quot;&gt;
    &lt;img src=&quot;/img/MongoDB-Replica-Set-Deployment-Architecture.png&quot; alt=&quot;Mongo Cluster Deployment Diagram&quot; /&gt;
&lt;/a&gt;
&lt;span class=&quot;caption text-muted&quot;&gt;AWS Architecture Diagram for the MongoDB Cluster Deployment&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Let's go over the resources that are going to be created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A VPC to isolate and contain all the instances&lt;/li&gt; 
&lt;li&gt;3 Public subnets and 3 Private subnets spread across 3 Availability Zones&lt;/li&gt;
&lt;li&gt;A Bastion Host (or Jump Box) for accessing the instances in the private subnets from an external network&lt;/li&gt;
&lt;li&gt;A NAT instance to provide internet access to the instances in the private subnets. (Side tip: NAT Gateway costs stack up a lot quickly than you can anticipate. Go for a NAT instance if your workloads aren't critical)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the source code is available at &lt;a href=&quot;https://github.com/itselavia/mongodb-terraform-deployment&quot;&gt;this GitHub repository.&lt;/a&gt; The code is structured as different Terraform modules such as VPC, JumpBox, MongoDB Cluster, etc. You can visit the repository for installation prerequisites and instructions on how to run the project. Here's a snippet of the main.tf file which utilizes all the Terraform modules:&lt;/p&gt;

&lt;p&gt;&amp;lt;?prettify?&amp;gt;&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
module &quot;vpc&quot; {
  source   = &quot;./vpc&quot;
  key_name = &quot;${module.key_pair.key_name}&quot;
  vpc_name = &quot;${var.vpc_name}&quot;
}

module &quot;key_pair&quot; {
  source     = &quot;./key_pair&quot;
  key_name   = &quot;mongo-key-pair&quot;
  public_key = &quot;${file(&quot;~/.ssh/id_rsa.pub&quot;)}&quot;
}

module &quot;jumpbox&quot; {
  source    = &quot;./jumpbox&quot;
  key_name  = &quot;${module.key_pair.key_name}&quot;
  vpc_id    = &quot;${module.vpc.vpc_id}&quot;
  subnet_id = &quot;${module.vpc.public_subnet_ids[1]}&quot;
}

module &quot;mongodb_cluster&quot; {
  source              = &quot;./mongodb_cluster&quot;
  key_name            = &quot;${module.key_pair.key_name}&quot;
  vpc_id              = &quot;${module.vpc.vpc_id}&quot;
  vpc_cidr_block      = &quot;${module.vpc.vpc_cidr_block}&quot;
  primary_node_type   = &quot;${var.primary_node_type}&quot;
  secondary_node_type = &quot;${var.secondary_node_type}&quot;
  private_subnet_ids  = &quot;${module.vpc.private_subnet_ids}&quot;
  jumpbox_public_ip   = &quot;${module.jumpbox.jumpbox_public_ip}&quot;
  replica_set_name    = &quot;${var.replica_set_name}&quot;
  num_secondary_nodes = &quot;${var.num_secondary_nodes}&quot;
  mongo_username      = &quot;${var.mongo_username}&quot;
  mongo_database      = &quot;${var.mongo_database}&quot;
  mongo_password      = &quot;${var.mongo_password}&quot;
}
&lt;/pre&gt;

&lt;p&gt;The MongoDB cluster module takes in a lot of configurable parameters which can be supplied by the terraform.tfvars file:&lt;/p&gt;

&lt;p&gt;&amp;lt;?prettify?&amp;gt;&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
vpc_name            = &quot;mongo_vpc&quot;
replica_set_name    = &quot;mongoRs&quot;
num_secondary_nodes = 3
mongo_username      = &quot;admin&quot;
mongo_password      = &quot;mongo4pass&quot;
mongo_database      = &quot;admin&quot;
primary_node_type   = &quot;t2.micro&quot;
secondary_node_type = &quot;t2.micro&quot;
&lt;/pre&gt;
&lt;h3 class=&quot;section-heading&quot;&gt;The MongoDB Cluster Module&lt;/h3&gt;
&lt;p&gt;We're going to deploy MongoDB v4.0 using the Ubuntu 18.04 AMI. &lt;a href=&quot;https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/&quot;&gt;This page from the official MongoDB documentation&lt;/a&gt; delineates steps to follow for installing MongoDB on Ubuntu. Just browsing through the insane number of steps involved or the confusing order of steps, you can imagine the dividend of scripting the commands rather than scampering through the documentation to figure out what worked the last time. Additionally, since this isn't a single node deployment, we need to deploy a replica set for MongoDB. &lt;a href=&quot;https://docs.mongodb.com/manual/replication/&quot;&gt;Here's excellent documentation&lt;/a&gt; which expands on the replication features of MongoDB. And &lt;a href=&quot;https://docs.mongodb.com/manual/tutorial/deploy-replica-set/&quot;&gt;here's the documentation&lt;/a&gt; specifying the steps needed to deploy a ReplicaSet&lt;/p&gt;

&lt;p&gt;The main.tf file for the MongoDB cluster module contains the code for creating the resources like EC2 instances, IAM policies, Security Groups, etc. Using Terraform's &lt;a href=&quot;https://www.terraform.io/docs/provisioners/file.html&quot;&gt;file provisioner&lt;/a&gt;, certain scripts and files (like the private key file needed to be shared by all the participating nodes) are uploaded to the instances while creating them. But wait, how can Terraform upload a file from your laptop to an instance launched in an AWS private subnet? Check out the nifty &quot;bastion_host&quot; parameter in the connection block: &lt;/p&gt;

&lt;p&gt;&amp;lt;?prettify?&amp;gt;&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
provisioner &quot;file&quot; {
    source      = &quot;${path.module}/keyFile&quot;
    destination = &quot;/home/ubuntu/keyFile&quot;

    connection {
      type         = &quot;ssh&quot;
      user         = &quot;ubuntu&quot;
      host         = &quot;${self.private_ip}&quot;
      agent        = false
      private_key  = &quot;${file(&quot;~/.ssh/id_rsa&quot;)}&quot;
      bastion_host = &quot;${var.jumpbox_public_ip}&quot;
      bastion_user = &quot;ec2-user&quot;
    }
  }
&lt;/pre&gt;

&lt;p&gt;Here's a snippet for creating the Secondary nodes. The &quot;count&quot; parameter (passed by the user) controls how many nodes to create. The instance is placed in any of the 3 private subnets in a round-robin manner.&lt;/p&gt;
&lt;p&gt;&amp;lt;?prettify?&amp;gt;&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
resource &quot;aws_instance&quot; &quot;mongo_secondary&quot; {
  count                  = &quot;${var.num_secondary_nodes}&quot;
  ami                    = &quot;${data.aws_ami.ubuntu_18_image.id}&quot;
  instance_type          = &quot;${var.secondary_node_type}&quot;
  key_name               = &quot;${var.key_name}&quot;
  subnet_id = &quot;${var.private_subnet_ids[count.index % length(var.private_subnet_ids)]}&quot;
  user_data              = &quot;${data.template_file.userdata.rendered}&quot;
  vpc_security_group_ids = [&quot;${aws_security_group.mongo_sg.id}&quot;]
  iam_instance_profile   = &quot;${aws_iam_instance_profile.mongo-instance-profile.name}&quot;
  root_block_device {
    volume_type = &quot;standard&quot;
  }

  tags = {
    Name = &quot;Mongo-Secondary_${count.index + 1}&quot;
    Type = &quot;secondary&quot;
  }
}
&lt;/pre&gt;

&lt;p&gt;The userdata shell script contains the bulk of the configuration. It scans the AWS environment for the cluster members and configures each instance with the private IPs of other members. This is accomplished by using the &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html&quot;&gt;EC2 Instance Metadata Service.&lt;/a&gt; Also, each instance has a Tag attached to it which helps to identify whether it's a Primary or Secondary node. Here are a few commands from the userdata shell script which creates MongoDB configuration and service files:&lt;/p&gt;

&lt;p&gt;&amp;lt;?prettify?&amp;gt;&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mongod.conf

cat &amp;gt;&amp;gt; /etc/mongod.conf &amp;lt;&amp;lt;EOL

security:
  keyFile: /opt/mongodb/keyFile

replication:
  replSetName: ${replica_set_name}

EOL

chown ubuntu:ubuntu /etc/mongod.conf

cat &amp;gt;&amp;gt; /etc/systemd/system/mongod.service &amp;lt;&amp;lt;EOL

[Unit]
Description=High-performance, schema-free document-oriented database
After=network.target

[Service]
User=mongodb
ExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf

[Install]
WantedBy=multi-user.target

EOL
&lt;/pre&gt;

&lt;p&gt;Go ahead and deploy the cluster in your AWS environment. Make sure you have a public-private keypair in the ~/.ssh directory. Use &lt;a href=&quot;https://www.ssh.com/ssh/keygen&quot;&gt;ssh-keygen&lt;/a&gt; to create the key files. Install &lt;a href=&quot;https://www.terraform.io/downloads.html&quot;&gt;Terraform&lt;/a&gt; and the &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux-mac.html&quot;&gt;AWS CLI&lt;/a&gt;. Create ~/.aws/credentials file to store your AWS Secret Key and Access Key locally for Terraform to use. I hope everything works great on the first try. If not, please feel free to reach out and I'll be more than happy to help!&lt;/p&gt;</content><author><name>Akshay Elavia</name></author><summary type="html">Deploying a cluster is hard. Deploying a database cluster is harder. Deploying a database cluster repeatedly to the cloud is an unimaginable headache. Nowadays, IaC tools allows teams to automatically provision the complex intertwined cloud infrastructure in a deterministic manner. There can be multiple reasons why you would want to invest efforts into writing code for automating the deployment of a database cluster: Performance testing your applications' access patterns by pointing them to your database Documenting the custom configurations made while setting up the database Safely and quickly replicating environments for staging, testing, DR, etc</summary></entry></feed>